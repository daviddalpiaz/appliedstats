<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Statistics with R</title>
  <meta name="description" content="Applied Statistics with <code>R</code>">
  <meta name="generator" content="bookdown 0.4.1 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Statistics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://daviddalpiaz.github.io/appliedstats/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/appliedstats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Statistics with R" />
  
  
  

<meta name="author" content="David Dalpiaz">


<meta name="date" content="2017-06-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="collinearity.html">
<link rel="next" href="beyond.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>1.1</b> About This Book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.2</b> Conventions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.4</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to <code>R</code></a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-started"><i class="fa fa-check"></i><b>2.1</b> Getting Started</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-calculations"><i class="fa fa-check"></i><b>2.2</b> Basic Calculations</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i><b>2.3</b> Getting Help</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-packages"><i class="fa fa-check"></i><b>2.4</b> Installing Packages</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-and-programming.html"><a href="data-and-programming.html"><i class="fa fa-check"></i><b>3</b> Data and Programming</a><ul>
<li class="chapter" data-level="3.1" data-path="data-and-programming.html"><a href="data-and-programming.html#data-types"><i class="fa fa-check"></i><b>3.1</b> Data Types</a></li>
<li class="chapter" data-level="3.2" data-path="data-and-programming.html"><a href="data-and-programming.html#data-structures"><i class="fa fa-check"></i><b>3.2</b> Data Structures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data-and-programming.html"><a href="data-and-programming.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-and-programming.html"><a href="data-and-programming.html#vectorization"><i class="fa fa-check"></i><b>3.2.2</b> Vectorization</a></li>
<li class="chapter" data-level="3.2.3" data-path="data-and-programming.html"><a href="data-and-programming.html#logical-operators"><i class="fa fa-check"></i><b>3.2.3</b> Logical Operators</a></li>
<li class="chapter" data-level="3.2.4" data-path="data-and-programming.html"><a href="data-and-programming.html#more-vectorization"><i class="fa fa-check"></i><b>3.2.4</b> More Vectorization</a></li>
<li class="chapter" data-level="3.2.5" data-path="data-and-programming.html"><a href="data-and-programming.html#matrices"><i class="fa fa-check"></i><b>3.2.5</b> Matrices</a></li>
<li class="chapter" data-level="3.2.6" data-path="data-and-programming.html"><a href="data-and-programming.html#lists"><i class="fa fa-check"></i><b>3.2.6</b> Lists</a></li>
<li class="chapter" data-level="3.2.7" data-path="data-and-programming.html"><a href="data-and-programming.html#data-frames"><i class="fa fa-check"></i><b>3.2.7</b> Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data-and-programming.html"><a href="data-and-programming.html#programming-basics"><i class="fa fa-check"></i><b>3.3</b> Programming Basics</a><ul>
<li class="chapter" data-level="3.3.1" data-path="data-and-programming.html"><a href="data-and-programming.html#control-flow"><i class="fa fa-check"></i><b>3.3.1</b> Control Flow</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-and-programming.html"><a href="data-and-programming.html#functions"><i class="fa fa-check"></i><b>3.3.2</b> Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing Data</a><ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#central-tendency"><i class="fa fa-check"></i>Central Tendency</a></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#spread"><i class="fa fa-check"></i>Spread</a></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical"><i class="fa fa-check"></i>Categorical</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#plotting"><i class="fa fa-check"></i><b>4.2</b> Plotting</a><ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#histograms"><i class="fa fa-check"></i><b>4.2.1</b> Histograms</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#barplots"><i class="fa fa-check"></i><b>4.2.2</b> Barplots</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#boxplots"><i class="fa fa-check"></i><b>4.2.3</b> Boxplots</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#scatterplots"><i class="fa fa-check"></i><b>4.2.4</b> Scatterplots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html"><i class="fa fa-check"></i><b>5</b> Probability and Statistics in <code>R</code></a><ul>
<li class="chapter" data-level="5.1" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#probability-in-r"><i class="fa fa-check"></i><b>5.1</b> Probability in <code>R</code></a><ul>
<li class="chapter" data-level="5.1.1" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#distributions"><i class="fa fa-check"></i><b>5.1.1</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#hypothesis-tests-in-r"><i class="fa fa-check"></i><b>5.2</b> Hypothesis Tests in <code>R</code></a><ul>
<li class="chapter" data-level="5.2.1" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#one-sample-t-test-review"><i class="fa fa-check"></i><b>5.2.1</b> One Sample t-Test: Review</a></li>
<li class="chapter" data-level="5.2.2" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#one-sample-t-test-example"><i class="fa fa-check"></i><b>5.2.2</b> One Sample t-Test: Example</a></li>
<li class="chapter" data-level="5.2.3" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#two-sample-t-test-review"><i class="fa fa-check"></i><b>5.2.3</b> Two Sample t-Test: Review</a></li>
<li class="chapter" data-level="5.2.4" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#two-sample-t-test-example"><i class="fa fa-check"></i><b>5.2.4</b> Two Sample t-Test: Example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#simulation"><i class="fa fa-check"></i><b>5.3</b> Simulation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#paired-differences"><i class="fa fa-check"></i><b>5.3.1</b> Paired Differences</a></li>
<li class="chapter" data-level="5.3.2" data-path="probability-and-statistics-in-r.html"><a href="probability-and-statistics-in-r.html#distribution-of-a-sample-mean"><i class="fa fa-check"></i><b>5.3.2</b> Distribution of a Sample Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="r-resources.html"><a href="r-resources.html"><i class="fa fa-check"></i><b>6</b> <code>R</code> Resources</a><ul>
<li class="chapter" data-level="6.1" data-path="r-resources.html"><a href="r-resources.html#beginner-tutorials-and-references"><i class="fa fa-check"></i><b>6.1</b> Beginner Tutorials and References</a></li>
<li class="chapter" data-level="6.2" data-path="r-resources.html"><a href="r-resources.html#intermediate-references"><i class="fa fa-check"></i><b>6.2</b> Intermediate References</a></li>
<li class="chapter" data-level="6.3" data-path="r-resources.html"><a href="r-resources.html#advanced-references"><i class="fa fa-check"></i><b>6.3</b> Advanced References</a></li>
<li class="chapter" data-level="6.4" data-path="r-resources.html"><a href="r-resources.html#rstudio-and-rmarkdown"><i class="fa fa-check"></i><b>6.4</b> RStudio and RMarkdown</a></li>
<li class="chapter" data-level="6.5" data-path="r-resources.html"><a href="r-resources.html#rmarkdown-template"><i class="fa fa-check"></i><b>6.5</b> RMarkdown Template</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>7</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#modeling"><i class="fa fa-check"></i><b>7.1</b> Modeling</a><ul>
<li class="chapter" data-level="7.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1.1</b> Simple Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-approach"><i class="fa fa-check"></i><b>7.2</b> Least Squares Approach</a><ul>
<li class="chapter" data-level="7.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#making-predictions"><i class="fa fa-check"></i><b>7.2.1</b> Making Predictions</a></li>
<li class="chapter" data-level="7.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals"><i class="fa fa-check"></i><b>7.2.2</b> Residuals</a></li>
<li class="chapter" data-level="7.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variance-estimation"><i class="fa fa-check"></i><b>7.2.3</b> Variance Estimation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#decomposition-of-variation"><i class="fa fa-check"></i><b>7.3</b> Decomposition of Variation</a><ul>
<li class="chapter" data-level="7.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>7.3.1</b> Coefficient of Determination</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-lm-function"><i class="fa fa-check"></i><b>7.4</b> The <code>lm</code> Function</a></li>
<li class="chapter" data-level="7.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#maximum-likelihood-estimation-mle-approach"><i class="fa fa-check"></i><b>7.5</b> Maximum Likelihood Estimation (MLE) Approach</a></li>
<li class="chapter" data-level="7.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simulating-slr"><i class="fa fa-check"></i><b>7.6</b> Simulating SLR</a></li>
<li class="chapter" data-level="7.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#history"><i class="fa fa-check"></i><b>7.7</b> History</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html"><i class="fa fa-check"></i><b>8</b> Inference for Simple Linear Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#gaussmarkov-theorem"><i class="fa fa-check"></i><b>8.1</b> Gauss–Markov Theorem</a></li>
<li class="chapter" data-level="8.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#sampling-distributions"><i class="fa fa-check"></i><b>8.2</b> Sampling Distributions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.2.1</b> Simulating Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#standard-errors"><i class="fa fa-check"></i><b>8.3</b> Standard Errors</a></li>
<li class="chapter" data-level="8.4" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-intervals-for-slope-and-intercept"><i class="fa fa-check"></i><b>8.4</b> Confidence Intervals for Slope and Intercept</a></li>
<li class="chapter" data-level="8.5" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#hypothesis-tests"><i class="fa fa-check"></i><b>8.5</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="8.6" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#cars-example"><i class="fa fa-check"></i><b>8.6</b> <code>cars</code> Example</a><ul>
<li class="chapter" data-level="8.6.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#tests-in-r"><i class="fa fa-check"></i><b>8.6.1</b> Tests in <code>R</code></a></li>
<li class="chapter" data-level="8.6.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#significance-of-regression-t-test"><i class="fa fa-check"></i><b>8.6.2</b> Significance of Regression, t-Test</a></li>
<li class="chapter" data-level="8.6.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-intervals-in-r"><i class="fa fa-check"></i><b>8.6.3</b> Confidence Intervals in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-interval-for-mean-response"><i class="fa fa-check"></i><b>8.7</b> Confidence Interval for Mean Response</a></li>
<li class="chapter" data-level="8.8" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#prediction-interval-for-new-observations"><i class="fa fa-check"></i><b>8.8</b> Prediction Interval for New Observations</a></li>
<li class="chapter" data-level="8.9" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-and-prediction-bands"><i class="fa fa-check"></i><b>8.9</b> Confidence and Prediction Bands</a></li>
<li class="chapter" data-level="8.10" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#significance-of-regression-f-test"><i class="fa fa-check"></i><b>8.10</b> Significance of Regression, F-Test</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>9</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#matrix-approach-to-regression"><i class="fa fa-check"></i><b>9.1</b> Matrix Approach to Regression</a></li>
<li class="chapter" data-level="9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sampling-distribution"><i class="fa fa-check"></i><b>9.2</b> Sampling Distribution</a><ul>
<li class="chapter" data-level="9.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-parameter-tests"><i class="fa fa-check"></i><b>9.2.1</b> Single Parameter Tests</a></li>
<li class="chapter" data-level="9.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>9.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="9.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#confidence-intervals-for-mean-response"><i class="fa fa-check"></i><b>9.2.3</b> Confidence Intervals for Mean Response</a></li>
<li class="chapter" data-level="9.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#prediction-intervals"><i class="fa fa-check"></i><b>9.2.4</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#significance-of-regression"><i class="fa fa-check"></i><b>9.3</b> Significance of Regression</a></li>
<li class="chapter" data-level="9.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#nested-models"><i class="fa fa-check"></i><b>9.4</b> Nested Models</a></li>
<li class="chapter" data-level="9.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simulation-1"><i class="fa fa-check"></i><b>9.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-building.html"><a href="model-building.html"><i class="fa fa-check"></i><b>10</b> Model Building</a><ul>
<li class="chapter" data-level="10.1" data-path="model-building.html"><a href="model-building.html#family-form-and-fit"><i class="fa fa-check"></i><b>10.1</b> Family, Form, and Fit</a><ul>
<li class="chapter" data-level="10.1.1" data-path="model-building.html"><a href="model-building.html#fit"><i class="fa fa-check"></i><b>10.1.1</b> Fit</a></li>
<li class="chapter" data-level="10.1.2" data-path="model-building.html"><a href="model-building.html#form"><i class="fa fa-check"></i><b>10.1.2</b> Form</a></li>
<li class="chapter" data-level="10.1.3" data-path="model-building.html"><a href="model-building.html#family"><i class="fa fa-check"></i><b>10.1.3</b> Family</a></li>
<li class="chapter" data-level="10.1.4" data-path="model-building.html"><a href="model-building.html#assumed-model-fitted-model"><i class="fa fa-check"></i><b>10.1.4</b> Assumed Model, Fitted Model</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="model-building.html"><a href="model-building.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>10.2</b> Explanation versus Prediction</a><ul>
<li class="chapter" data-level="10.2.1" data-path="model-building.html"><a href="model-building.html#explanation"><i class="fa fa-check"></i><b>10.2.1</b> Explanation</a></li>
<li class="chapter" data-level="10.2.2" data-path="model-building.html"><a href="model-building.html#prediction"><i class="fa fa-check"></i><b>10.2.2</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="model-building.html"><a href="model-building.html#summary"><i class="fa fa-check"></i><b>10.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html"><i class="fa fa-check"></i><b>11</b> Categorical Predictors and Interactions</a><ul>
<li class="chapter" data-level="11.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#dummy-variables"><i class="fa fa-check"></i><b>11.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="11.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#interactions"><i class="fa fa-check"></i><b>11.2</b> Interactions</a></li>
<li class="chapter" data-level="11.3" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#factor-variables"><i class="fa fa-check"></i><b>11.3</b> Factor Variables</a><ul>
<li class="chapter" data-level="11.3.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>11.3.1</b> Factors with More Than Two Levels</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#parameterization"><i class="fa fa-check"></i><b>11.4</b> Parameterization</a></li>
<li class="chapter" data-level="11.5" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#building-larger-models"><i class="fa fa-check"></i><b>11.5</b> Building Larger Models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>12</b> Model Diagnostics</a><ul>
<li class="chapter" data-level="12.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#model-assumptions"><i class="fa fa-check"></i><b>12.1</b> Model Assumptions</a></li>
<li class="chapter" data-level="12.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-assumptions"><i class="fa fa-check"></i><b>12.2</b> Checking Assumptions</a><ul>
<li class="chapter" data-level="12.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#fitted-versus-residuals-plot"><i class="fa fa-check"></i><b>12.2.1</b> Fitted versus Residuals Plot</a></li>
<li class="chapter" data-level="12.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#breusch-pagan-test"><i class="fa fa-check"></i><b>12.2.2</b> Breusch-Pagan Test</a></li>
<li class="chapter" data-level="12.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#histograms-1"><i class="fa fa-check"></i><b>12.2.3</b> Histograms</a></li>
<li class="chapter" data-level="12.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#q-q-plots"><i class="fa fa-check"></i><b>12.2.4</b> Q-Q Plots</a></li>
<li class="chapter" data-level="12.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#shapiro-wilk-test"><i class="fa fa-check"></i><b>12.2.5</b> Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#unusual-observations"><i class="fa fa-check"></i><b>12.3</b> Unusual Observations</a><ul>
<li class="chapter" data-level="12.3.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#leverage"><i class="fa fa-check"></i><b>12.3.1</b> Leverage</a></li>
<li class="chapter" data-level="12.3.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#outliers"><i class="fa fa-check"></i><b>12.3.2</b> Outliers</a></li>
<li class="chapter" data-level="12.3.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#influence"><i class="fa fa-check"></i><b>12.3.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#data-analysis-examples"><i class="fa fa-check"></i><b>12.4</b> Data Analysis Examples</a><ul>
<li class="chapter" data-level="12.4.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#good-diagnostics"><i class="fa fa-check"></i><b>12.4.1</b> Good Diagnostics</a></li>
<li class="chapter" data-level="12.4.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#suspect-diagnostics"><i class="fa fa-check"></i><b>12.4.2</b> Suspect Diagnostics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>13</b> Transformations</a><ul>
<li class="chapter" data-level="13.1" data-path="transformations.html"><a href="transformations.html#response-transformation"><i class="fa fa-check"></i><b>13.1</b> Response Transformation</a><ul>
<li class="chapter" data-level="13.1.1" data-path="transformations.html"><a href="transformations.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>13.1.1</b> Variance Stabilizing Transformations</a></li>
<li class="chapter" data-level="13.1.2" data-path="transformations.html"><a href="transformations.html#box-cox-transformations"><i class="fa fa-check"></i><b>13.1.2</b> Box-Cox Transformations</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="transformations.html"><a href="transformations.html#predictor-transformation"><i class="fa fa-check"></i><b>13.2</b> Predictor Transformation</a><ul>
<li class="chapter" data-level="13.2.1" data-path="transformations.html"><a href="transformations.html#polynomials"><i class="fa fa-check"></i><b>13.2.1</b> Polynomials</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>14</b> Collinearity</a><ul>
<li class="chapter" data-level="14.1" data-path="collinearity.html"><a href="collinearity.html#exact-collinearity"><i class="fa fa-check"></i><b>14.1</b> Exact Collinearity</a></li>
<li class="chapter" data-level="14.2" data-path="collinearity.html"><a href="collinearity.html#collinearity-1"><i class="fa fa-check"></i><b>14.2</b> Collinearity</a><ul>
<li class="chapter" data-level="14.2.1" data-path="collinearity.html"><a href="collinearity.html#variance-inflation-factor."><i class="fa fa-check"></i><b>14.2.1</b> Variance Inflation Factor.</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="collinearity.html"><a href="collinearity.html#simulation-2"><i class="fa fa-check"></i><b>14.3</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html"><i class="fa fa-check"></i><b>15</b> Variable Selection and Model Building</a><ul>
<li class="chapter" data-level="15.1" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#quality-criterion"><i class="fa fa-check"></i><b>15.1</b> Quality Criterion</a><ul>
<li class="chapter" data-level="15.1.1" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#akaike-information-criterion"><i class="fa fa-check"></i><b>15.1.1</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="15.1.2" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#bayesian-information-criterion"><i class="fa fa-check"></i><b>15.1.2</b> Bayesian Information Criterion</a></li>
<li class="chapter" data-level="15.1.3" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#adjusted-r-squared"><i class="fa fa-check"></i><b>15.1.3</b> Adjusted R-Squared</a></li>
<li class="chapter" data-level="15.1.4" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#cross-validated-rmse"><i class="fa fa-check"></i><b>15.1.4</b> Cross-Validated RMSE</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#selection-procedures"><i class="fa fa-check"></i><b>15.2</b> Selection Procedures</a><ul>
<li class="chapter" data-level="15.2.1" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#backward-search"><i class="fa fa-check"></i><b>15.2.1</b> Backward Search</a></li>
<li class="chapter" data-level="15.2.2" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#forward-search"><i class="fa fa-check"></i><b>15.2.2</b> Forward Search</a></li>
<li class="chapter" data-level="15.2.3" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#stepwise-search"><i class="fa fa-check"></i><b>15.2.3</b> Stepwise Search</a></li>
<li class="chapter" data-level="15.2.4" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#exhaustive-search"><i class="fa fa-check"></i><b>15.2.4</b> Exhaustive Search</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#higher-order-terms"><i class="fa fa-check"></i><b>15.3</b> Higher Order Terms</a></li>
<li class="chapter" data-level="15.4" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#explanation-versus-prediction-1"><i class="fa fa-check"></i><b>15.4</b> Explanation versus Prediction</a><ul>
<li class="chapter" data-level="15.4.1" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#explanation-1"><i class="fa fa-check"></i><b>15.4.1</b> Explanation</a></li>
<li class="chapter" data-level="15.4.2" data-path="variable-selection-and-model-building.html"><a href="variable-selection-and-model-building.html#prediction-1"><i class="fa fa-check"></i><b>15.4.2</b> Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="beyond.html"><a href="beyond.html"><i class="fa fa-check"></i><b>16</b> Beyond</a><ul>
<li class="chapter" data-level="16.1" data-path="beyond.html"><a href="beyond.html#whats-next"><i class="fa fa-check"></i><b>16.1</b> What’s Next</a></li>
<li class="chapter" data-level="16.2" data-path="beyond.html"><a href="beyond.html#rstudio"><i class="fa fa-check"></i><b>16.2</b> RStudio</a></li>
<li class="chapter" data-level="16.3" data-path="beyond.html"><a href="beyond.html#tidy-data"><i class="fa fa-check"></i><b>16.3</b> Tidy Data</a></li>
<li class="chapter" data-level="16.4" data-path="beyond.html"><a href="beyond.html#visualization"><i class="fa fa-check"></i><b>16.4</b> Visualization</a></li>
<li class="chapter" data-level="16.5" data-path="beyond.html"><a href="beyond.html#web-applications"><i class="fa fa-check"></i><b>16.5</b> Web Applications</a></li>
<li class="chapter" data-level="16.6" data-path="beyond.html"><a href="beyond.html#experimental-design"><i class="fa fa-check"></i><b>16.6</b> Experimental Design</a></li>
<li class="chapter" data-level="16.7" data-path="beyond.html"><a href="beyond.html#machine-learning"><i class="fa fa-check"></i><b>16.7</b> Machine Learning</a><ul>
<li class="chapter" data-level="16.7.1" data-path="beyond.html"><a href="beyond.html#deep-learning"><i class="fa fa-check"></i><b>16.7.1</b> Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="beyond.html"><a href="beyond.html#time-series"><i class="fa fa-check"></i><b>16.8</b> Time Series</a></li>
<li class="chapter" data-level="16.9" data-path="beyond.html"><a href="beyond.html#bayesianism"><i class="fa fa-check"></i><b>16.9</b> Bayesianism</a></li>
<li class="chapter" data-level="16.10" data-path="beyond.html"><a href="beyond.html#high-performance-computing"><i class="fa fa-check"></i><b>16.10</b> High Performance Computing</a></li>
<li class="chapter" data-level="16.11" data-path="beyond.html"><a href="beyond.html#further-r-resources"><i class="fa fa-check"></i><b>16.11</b> Further <code>R</code> Resources</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="transformations-video.html"><a href="transformations-video.html"><i class="fa fa-check"></i><b>17</b> Transformations Video</a><ul>
<li class="chapter" data-level="17.1" data-path="transformations-video.html"><a href="transformations-video.html#response-transformations"><i class="fa fa-check"></i><b>17.1</b> Response Transformations</a></li>
<li class="chapter" data-level="17.2" data-path="transformations-video.html"><a href="transformations-video.html#predictor-transformations"><i class="fa fa-check"></i><b>17.2</b> Predictor Transformations</a><ul>
<li class="chapter" data-level="17.2.1" data-path="transformations-video.html"><a href="transformations-video.html#a-quadratic-model"><i class="fa fa-check"></i><b>17.2.1</b> A Quadratic Model</a></li>
<li class="chapter" data-level="17.2.2" data-path="transformations-video.html"><a href="transformations-video.html#overfitting-and-extrapolation"><i class="fa fa-check"></i><b>17.2.2</b> Overfitting and Extrapolation</a></li>
<li class="chapter" data-level="17.2.3" data-path="transformations-video.html"><a href="transformations-video.html#comparing-polynomial-models"><i class="fa fa-check"></i><b>17.2.3</b> Comparing Polynomial Models</a></li>
<li class="chapter" data-level="17.2.4" data-path="transformations-video.html"><a href="transformations-video.html#poly-function-and-orthogonal-polynomials"><i class="fa fa-check"></i><b>17.2.4</b> <code>poly()</code> Function and Orthogonal Polynomials</a></li>
<li class="chapter" data-level="17.2.5" data-path="transformations-video.html"><a href="transformations-video.html#inhibit-function"><i class="fa fa-check"></i><b>17.2.5</b> Inhibit Function</a></li>
<li class="chapter" data-level="17.2.6" data-path="transformations-video.html"><a href="transformations-video.html#data-example"><i class="fa fa-check"></i><b>17.2.6</b> Data Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/daviddalpiaz/appliedstats" target="blank">&copy; 2016 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Statistics with <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variable-selection-and-model-building" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Variable Selection and Model Building</h1>
<blockquote>
<p>“Choose well. Your choice is brief, and yet endless.”</p>
<p>— <strong>Johann Wolfgang von Goethe</strong></p>
</blockquote>
<p>After reading this chapter you will be able to:</p>
<ul>
<li>Understand the trade-off between goodness-of-fit and model complexity.</li>
<li>Use variable selection procedures to find a good model from a set of possible models.</li>
<li>Understand the two uses of models: explanation and prediction.</li>
</ul>
<p>Last chapter we saw how correlation between predictor variables can have undesirable effects on models. We used variance inflation factors to assess the severity of the collinearity issues caused by these correlations. We also saw how fitting a smaller model, leaving out some of the correlated predictors, results in a model which no longer suffers from collinearity issues. But how should we chose this smaller model?</p>
<p>This chapter, we will discuss several <em>criteria</em> and <em>procedures</em> for choosing a “good” model from among a choice of many.</p>
<div id="quality-criterion" class="section level2">
<h2><span class="header-section-number">15.1</span> Quality Criterion</h2>
<p>So far, we have seen criteria such as <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\text{RMSE}\)</span> for assessing quality of fit. However, both of these have a fatal flaw. By increasing the size of a model, that is adding predictors, that can at worst not improve. It is impossible to add a predictor to a model and make <span class="math inline">\(R^2\)</span> or <span class="math inline">\(\text{RMSE}\)</span> worse. That means, if we were to use either of these to chose between models, we would <em>always</em> simply choose the larger model. Eventually we would simply be fitting to noise.</p>
<p>This suggests that we need a quality criteria that takes into account the size of the model, since our preference is for small models that still fit well. We are willing to sacrifice a small amount of goodness-of-fit for obtaining a smaller model. We will look at three criteria that do this explicitly: <span class="math inline">\(\text{AIC}\)</span>, <span class="math inline">\(\text{BIC}\)</span>, and Adjusted <span class="math inline">\(R^2\)</span>. We will also look at one, Cross-Validated <span class="math inline">\(\text{RMSE}\)</span>, which implicitly considers the size of the model.</p>
<div id="akaike-information-criterion" class="section level3">
<h3><span class="header-section-number">15.1.1</span> Akaike Information Criterion</h3>
<p>The first criteria we will discuss is the Akaike Information Criterion, or <span class="math inline">\(\text{AIC}\)</span> for short. (Note that, when <em>Akaike</em> first introduced this metric, it was simply called <em>An</em> Information Criterion. The <em>A</em> has changed meaning over the years.)</p>
<p>Recall, the maximized log-likelihood of a regression model can be written as</p>
<p><span class="math display">\[
\log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\left(\frac{\text{RSS}}{n}\right) - \frac{n}{2},
\]</span></p>
<p>where <span class="math inline">\(\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i) ^ 2\)</span> and <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span> were chosen to maximize the likelihood.</p>
<p>Then we can define <span class="math inline">\(\text{AIC}\)</span> as</p>
<p><span class="math display">\[
\text{AIC} = -2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) + 2p = n + n \log(2\pi) + n \log\left(\frac{\text{RSS}}{n}\right) + 2p,
\]</span></p>
<p>which is a measure of quality of the model. The smaller the <span class="math inline">\(\text{AIC}\)</span>, the better. To see why, let’s talk about the two main components of <span class="math inline">\(\text{AIC}\)</span>, the <strong>likelihood</strong> (which measures goodness-of-fit) and the <strong>penalty</strong> (which is a function of the size of the model).</p>
<p>The likelihood portion of <span class="math inline">\(\text{AIC}\)</span> is given by</p>
<p><span class="math display">\[
-2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) = n + n \log(2\pi) + n \log\left(\frac{\text{RSS}}{n}\right).
\]</span></p>
<p>For the sake of comparing models, the only term here that will change is <span class="math inline">\(n \log\left(\frac{\text{RSS}}{n}\right)\)</span>, which is function of <span class="math inline">\(\text{RSS}\)</span>. The</p>
<p><span class="math display">\[
n + n \log(2\pi)
\]</span></p>
<p>terms will be constant across all models applied to the same data. So, when a model fits well, that is, has a low <span class="math inline">\(\text{RSS}\)</span>, then this likelihood component will be small.</p>
<p>Similarly, we can discuss the penalty component of <span class="math inline">\(\text{AIC}\)</span> which is,</p>
<p><span class="math display">\[
2p,
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of <span class="math inline">\(\beta\)</span> parameters in the model. We call this a penalty, because it is large when <span class="math inline">\(p\)</span> is large, but we are seeking to find a small <span class="math inline">\(\text{AIC}\)</span></p>
<p>Thus, a good model, that is one with a small <span class="math inline">\(\text{AIC}\)</span>, will have a good balance between fitting well, and using a small number of parameters. For comparing models</p>
<p><span class="math display">\[
\text{AIC} = n\log\left(\frac{\text{RSS}}{n}\right) + 2p
\]</span></p>
<p>is a sufficient expression, as <span class="math inline">\(n + n \log(2\pi)\)</span> is the same across all models for any particular dataset.</p>
</div>
<div id="bayesian-information-criterion" class="section level3">
<h3><span class="header-section-number">15.1.2</span> Bayesian Information Criterion</h3>
<p>The Bayesian Information Criterion, or <span class="math inline">\(\text{BIC}\)</span>, is similar to <span class="math inline">\(\text{AIC}\)</span>, but has a larger penalty. <span class="math inline">\(\text{BIC}\)</span> also quantifies the trade-off between a model which fits well and the number of model parameters, however for a reasonable sample size, generally picks a smaller model than <span class="math inline">\(\text{AIC}\)</span>. Again, for model selection use the model with the smallest <span class="math inline">\(\text{BIC}\)</span>.</p>
<p><span class="math display">\[
\text{BIC} = -2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) + \log(n) p = n + n\log(2\pi) + n\log\left(\frac{\text{RSS}}{n}\right) + \log(n)p.
\]</span></p>
<p>Notice that the <span class="math inline">\(\text{AIC}\)</span> penalty was</p>
<p><span class="math display">\[
2p,
\]</span></p>
<p>whereas for <span class="math inline">\(\text{BIC}\)</span>, the penalty is</p>
<p><span class="math display">\[
\log(n) p.
\]</span></p>
<p>So, for any dataset where <span class="math inline">\(log(n) &gt; 2\)</span> the <span class="math inline">\(\text{BIC}\)</span> penalty will be larger than the <span class="math inline">\(\text{AIC}\)</span> penalty, thus <span class="math inline">\(\text{BIC}\)</span> will likely prefer a smaller model.</p>
<p>Note that, sometimes the penalty is considered a general expression of the form</p>
<p><span class="math display">\[
k \cdot p.
\]</span></p>
<p>Then, for <span class="math inline">\(\text{AIC}\)</span> <span class="math inline">\(k = 2\)</span>, and for <span class="math inline">\(\text{BIC}\)</span> <span class="math inline">\(k = \log(n)\)</span>.</p>
<p>For comparing models</p>
<p><span class="math display">\[
\text{BIC} = n\log\left(\frac{\text{RSS}}{n}\right) + \log(n)p
\]</span></p>
<p>is again a sufficient expression, as <span class="math inline">\(n + n \log(2\pi)\)</span> is the same across all models for any particular dataset.</p>
</div>
<div id="adjusted-r-squared" class="section level3">
<h3><span class="header-section-number">15.1.3</span> Adjusted R-Squared</h3>
<p>Recall,</p>
<p><span class="math display">\[
R^2 = 1 - \frac{\text{SSE}}{\text{SST}} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}.
\]</span></p>
<p>We now define</p>
<p><span class="math display">\[
R_a^2 = 1 - \frac{\text{SSE}/(n-p)}{\text{SST}/(n-1)} = 1 - \left(  \frac{n-1}{n-p} \right)(1-R^2)
\]</span></p>
<p>which we call the Adjusted <span class="math inline">\(R^2\)</span>.</p>
<p>Unlike <span class="math inline">\(R^2\)</span> which can never become smaller with added predictors, Adjusted <span class="math inline">\(R^2\)</span> effectively penalizes for additional predictors, and can decrease with added predictors. Like <span class="math inline">\(R^2\)</span>, larger is still better.</p>
</div>
<div id="cross-validated-rmse" class="section level3">
<h3><span class="header-section-number">15.1.4</span> Cross-Validated RMSE</h3>
<p>Each of the previous three metrics explicitly used <span class="math inline">\(p\)</span>, the number of parameters, in their calculations. Thus, they all explicitly limit the size of models chosen when used to compare models.</p>
<p>We’ll now briefly introduce <strong>overfitting</strong> and <strong>cross-validation</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make_poly_data =<span class="st"> </span>function(<span class="dt">sample_size =</span> <span class="dv">11</span>) {
  x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>)
  y =<span class="st"> </span><span class="dv">3</span> +<span class="st"> </span>x +<span class="st"> </span><span class="dv">4</span> *<span class="st"> </span>x ^<span class="st"> </span><span class="dv">2</span> +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">20</span>)
  <span class="kw">data.frame</span>(x, y)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
poly_data =<span class="st"> </span><span class="kw">make_poly_data</span>()</code></pre></div>
<p>Here we have generated data where the mean of <span class="math inline">\(Y\)</span> is a quadratic function of a single predictor <span class="math inline">\(x\)</span>, specifically,</p>
<p><span class="math display">\[
Y = 3 + x + 4 x ^ 2 + \epsilon.
\]</span></p>
<p>We’ll now fit two models to this data, one which has the correct form, quadratic, and one that is large, which includes terms up to and including an eighth degree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_quad =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> poly_data)
fit_big  =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">8</span>), <span class="dt">data =</span> poly_data)</code></pre></div>
<p>We then plot the data and the results of the two models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> poly_data, <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="dv">100</span>, <span class="dv">400</span>), <span class="dt">cex =</span> <span class="dv">2</span>, <span class="dt">pch =</span> <span class="dv">20</span>)
xplot =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">by =</span> <span class="fl">0.1</span>)
<span class="kw">lines</span>(xplot, <span class="kw">predict</span>(fit_quad, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> xplot)),
      <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
<span class="kw">lines</span>(xplot, <span class="kw">predict</span>(fit_big, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> xplot)),
      <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-497-1.png" width="672" /></p>
<p>We can see that the solid blue curve models this data rather nicely. The dashed orange curve fits the points better, making smaller errors, however it is unlikely that it is correctly modeling the true relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. It is fitting the random noise. This is an example of <strong>overfitting</strong>.</p>
<p>We see that the larger model indeed has a lower <span class="math inline">\(\text{RMSE}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">resid</span>(fit_quad) ^<span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 17.61812</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">resid</span>(fit_big) ^<span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 10.4197</code></pre>
<p>To correct for this, we will introduce cross-validation. We define the leave-one-out cross-validated RMSE to be</p>
<p><span class="math display">\[
\text{RMSE}_{\text{LOOCV}} = \sqrt{\frac{1}{n} \sum_{i=1}^n e_{[i]}^2}.
\]</span></p>
<p>The <span class="math inline">\(e_{[i]}\)</span> are the residual for the <span class="math inline">\(i\)</span>th observation, when that observation is <strong>not</strong> used to fit the model.</p>
<p><span class="math display">\[
{e_{[i]} = y_{i} – \hat{y}_{[i]}}
\]</span></p>
<p>That is, the fitted value is calculated as</p>
<p><span class="math display">\[
\hat{y}_{[i]} = x_i ^ \top \hat{\beta}_{[i]}
\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_{[i]}\)</span> are the estimated coefficients when the <span class="math inline">\(i\)</span>th observation is removed from the dataset.</p>
<p>In general, to perform this calculation, we would be required to fit the model <span class="math inline">\(n\)</span> times, once with each possible observation removed. However, for leave-one-out cross-validation and linear models, the equation can be rewritten as</p>
<p><span class="math display">\[
\text{RMSE}_{\text{LOOCV}} = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(\frac{e_{i}}{1-h_{i}}\right)^2},
\]</span></p>
<p>where <span class="math inline">\(h_i\)</span> are the leverages and <span class="math inline">\(e_i\)</span> are the usual residuals. This is great, because now we can obtain the LOOCV <span class="math inline">\(\text{RMSE}\)</span> by fitting only one model! In practice 5 or 10 fold cross-validation are much more popular. For example, in 5-fold cross-validation, the model is fit 5 times, each time leaving out a fifth of the data, then predicting on those values. We’ll leave in-depth examination of cross-validation to a machine learning course, and simply use LOOCV here.</p>
<p>Let’s calculate LOOCV <span class="math inline">\(\text{RMSE}\)</span> for both models, then discuss <em>why</em> we want to do so. We first write a function which calculates the LOOCV <span class="math inline">\(\text{RMSE}\)</span> as defined using the shortcut formula for linear models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">calc_loocv_rmse =<span class="st"> </span>function(model) {
  <span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">resid</span>(model) /<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw">hatvalues</span>(model))) ^<span class="st"> </span><span class="dv">2</span>))
}</code></pre></div>
<p>Then calculate the metric for both models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(fit_quad)</code></pre></div>
<pre><code>## [1] 23.57189</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(fit_big)</code></pre></div>
<pre><code>## [1] 1334.357</code></pre>
<p>Now we see that the quadratic model has a much smaller LOOCV <span class="math inline">\(\text{RMSE}\)</span>, so we would prefer this quadratic model. This is because the large model has <em>severely</em> over-fit the data. By leaving a single data point out and fitting the large model, the resulting fit is much different than the fit using all of the data. For example, let’s leave out the third data point and fit both models, then plot the result.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_quad_removed =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> poly_data[-<span class="dv">3</span>, ])
fit_big_removed  =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">8</span>), <span class="dt">data =</span> poly_data[-<span class="dv">3</span>, ])

<span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> poly_data, <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="dv">100</span>, <span class="dv">400</span>), <span class="dt">cex =</span> <span class="dv">2</span>, <span class="dt">pch =</span> <span class="dv">20</span>)
xplot =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">by =</span> <span class="fl">0.1</span>)
<span class="kw">lines</span>(xplot, <span class="kw">predict</span>(fit_quad_removed, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> xplot)),
      <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
<span class="kw">lines</span>(xplot, <span class="kw">predict</span>(fit_big_removed, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> xplot)),
      <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-501-1.png" width="672" /></p>
<p>We see that on average, the solid blue line for the quadratic model has similar errors as before. It has changed very slightly. However, the dashed orange line for the large model, has a huge error at the point that was removed and is much different that the previous fit.</p>
<p>This is the purpose of cross-validation. By assessing how the model fits points that were not used to perform the regression, we get an idea of how well the model will work for future observations. It assess how well the model works in general, not simply on the observed data.</p>
</div>
</div>
<div id="selection-procedures" class="section level2">
<h2><span class="header-section-number">15.2</span> Selection Procedures</h2>
<p>We’ve now seen a number of model quality criteria, but now we need to address which models to consider. Model selection involves both a quality criterion, plus a search procedure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
hipcenter_mod =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span>., <span class="dt">data =</span> seatpos)
<span class="kw">coef</span>(hipcenter_mod)</code></pre></div>
<pre><code>##  (Intercept)          Age       Weight      HtShoes           Ht       Seated 
## 436.43212823   0.77571620   0.02631308  -2.69240774   0.60134458   0.53375170 
##          Arm        Thigh          Leg 
##  -1.32806864  -1.14311888  -6.43904627</code></pre>
<p>Let’s return to the <code>seatpos</code> data from the <code>faraway</code> package. Now, let’s consider only models with first order terms, thus no interactions and no polynomials. There are <em>eight</em> predictors in this model. So if we consider all possible models, ranging from using 0 predictors, to all eight predictors, there are</p>
<p><span class="math display">\[
\sum_{k = 0}^{p - 1} {{p - 1} \choose {k}} = 2 ^ {p - 1} = 2 ^ 8 = 256
\]</span></p>
<p>possible models.</p>
<p>If we had 10 or more predictors, we would already be considering over 1000 models! For this reason, we often search through possible models in an intelligent way, bypassing some models that are unlikely to be considered good. We will consider three search procedures: backwards, forwards, and stepwise.</p>
<div id="backward-search" class="section level3">
<h3><span class="header-section-number">15.2.1</span> Backward Search</h3>
<p>Backward selection procedures start with all possible predictors in the model, then considers how deleting a single predictor will effect a chosen metric. Let’s try this on the <code>seatpos</code> data. We will use the <code>step()</code> function in <code>R</code> which by default uses <span class="math inline">\(\text{AIC}\)</span> as its metric of choice.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_back_aic =<span class="st"> </span><span class="kw">step</span>(hipcenter_mod, <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>)</code></pre></div>
<pre><code>## Start:  AIC=283.62
## hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + 
##     Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Ht       1      5.01 41267 281.63
## - Weight   1      8.99 41271 281.63
## - Seated   1     28.64 41290 281.65
## - HtShoes  1    108.43 41370 281.72
## - Arm      1    164.97 41427 281.78
## - Thigh    1    262.76 41525 281.87
## &lt;none&gt;                 41262 283.62
## - Age      1   2632.12 43894 283.97
## - Leg      1   2654.85 43917 283.99
## 
## Step:  AIC=281.63
## hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Weight   1     11.10 41278 279.64
## - Seated   1     30.52 41297 279.66
## - Arm      1    160.50 41427 279.78
## - Thigh    1    269.08 41536 279.88
## - HtShoes  1    971.84 42239 280.51
## &lt;none&gt;                 41267 281.63
## - Leg      1   2664.65 43931 282.01
## - Age      1   2808.52 44075 282.13
## 
## Step:  AIC=279.64
## hipcenter ~ Age + HtShoes + Seated + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Seated   1     35.10 41313 277.67
## - Arm      1    156.47 41434 277.78
## - Thigh    1    285.16 41563 277.90
## - HtShoes  1    975.48 42253 278.53
## &lt;none&gt;                 41278 279.64
## - Leg      1   2661.39 43939 280.01
## - Age      1   3011.86 44290 280.31
## 
## Step:  AIC=277.67
## hipcenter ~ Age + HtShoes + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Arm      1    172.02 41485 275.83
## - Thigh    1    344.61 41658 275.99
## - HtShoes  1   1853.43 43166 277.34
## &lt;none&gt;                 41313 277.67
## - Leg      1   2871.07 44184 278.22
## - Age      1   2976.77 44290 278.31
## 
## Step:  AIC=275.83
## hipcenter ~ Age + HtShoes + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Thigh    1     472.8 41958 274.26
## &lt;none&gt;                 41485 275.83
## - HtShoes  1    2340.7 43826 275.92
## - Age      1    3501.0 44986 276.91
## - Leg      1    3591.7 45077 276.98
## 
## Step:  AIC=274.26
## hipcenter ~ Age + HtShoes + Leg
## 
##           Df Sum of Sq   RSS    AIC
## &lt;none&gt;                 41958 274.26
## - Age      1    3108.8 45067 274.98
## - Leg      1    3476.3 45434 275.28
## - HtShoes  1    4218.6 46176 275.90</code></pre>
<p>We start with the model <code>hipcenter ~ .</code>, which is otherwise known as <code>hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg</code>. <code>R</code> will then repeatedly attempt to delete a predictor until it stops, or reaches the model <code>hipcenter ~ 1</code>, which contains no predictors.</p>
<p>At each “step”, <code>R</code> reports the current model, its <span class="math inline">\(\text{AIC}\)</span>, and the possible steps with their <span class="math inline">\(\text{RSS}\)</span> and more importantly <span class="math inline">\(\text{AIC}\)</span>.</p>
<p>In this example, at the first step, the current model is <code>hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg</code> which has an AIC of <code>283.62</code>. Note that when <code>R</code> is calculating this value, it is using <code>extractAIC()</code>, which uses the expression</p>
<p><span class="math display">\[
\text{AIC} = n\log\left(\frac{\text{RSS}}{n}\right) + 2p,
\]</span></p>
<p>which we quickly verify.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod) <span class="co"># returns both p and AIC</span></code></pre></div>
<pre><code>## [1]   9.000 283.624</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="kw">length</span>(<span class="kw">resid</span>(hipcenter_mod))
(<span class="dt">p =</span> <span class="kw">length</span>(<span class="kw">coef</span>(hipcenter_mod)))</code></pre></div>
<pre><code>## [1] 9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n *<span class="st"> </span><span class="kw">log</span>(<span class="kw">mean</span>(<span class="kw">resid</span>(hipcenter_mod) ^<span class="st"> </span><span class="dv">2</span>)) +<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>p</code></pre></div>
<pre><code>## [1] 283.624</code></pre>
<p>Returning to the first step, <code>R</code> then gives us a row which shows the effect of deleting each of the current predictors. The <code>-</code> signs at the beginning of each row indicates we are considering removing a predictor. There is also a row with <code>&lt;none&gt;</code> which is a row for keeping the current model. Notice that this row has the smallest <span class="math inline">\(\text{RSS}\)</span>, as it is the largest model.</p>
<p>We see that every row above <code>&lt;none&gt;</code> has a smaller <span class="math inline">\(\text{AIC}\)</span> than the row for <code>&lt;none&gt;</code> with the one at the top, <code>Ht</code>, giving the lowest <span class="math inline">\(\text{AIC}\)</span>. Thus we remove <code>Ht</code> from the model, and continue the process.</p>
<p>Notice, in the second step, we start with the model <code>hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg</code> and the variable <code>Ht</code> is no longer considered.</p>
<p>We continue the process until we reach the model <code>hipcenter ~ Age + HtShoes + Leg</code>. At this step, the row for <code>&lt;none&gt;</code> tops the list, as removing any additional variable will not improve the <span class="math inline">\(\text{AIC}\)</span> This is the model which is stored in <code>hipcenter_mod_back_aic</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(hipcenter_mod_back_aic)</code></pre></div>
<pre><code>## (Intercept)         Age     HtShoes         Leg 
## 456.2136538   0.5998327  -2.3022555  -6.8297461</code></pre>
<p>We could also search through the possible models in a backwards fashion using <span class="math inline">\(\text{BIC}\)</span>. To do so, we again use the <code>step()</code> function, but now specify <code>k = log(n)</code>, where <code>n</code> stores the number of observations in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="kw">length</span>(<span class="kw">resid</span>(hipcenter_mod))
hipcenter_mod_back_bic =<span class="st"> </span><span class="kw">step</span>(hipcenter_mod, <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## Start:  AIC=298.36
## hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + 
##     Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Ht       1      5.01 41267 294.73
## - Weight   1      8.99 41271 294.73
## - Seated   1     28.64 41290 294.75
## - HtShoes  1    108.43 41370 294.82
## - Arm      1    164.97 41427 294.88
## - Thigh    1    262.76 41525 294.97
## - Age      1   2632.12 43894 297.07
## - Leg      1   2654.85 43917 297.09
## &lt;none&gt;                 41262 298.36
## 
## Step:  AIC=294.73
## hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Weight   1     11.10 41278 291.10
## - Seated   1     30.52 41297 291.12
## - Arm      1    160.50 41427 291.24
## - Thigh    1    269.08 41536 291.34
## - HtShoes  1    971.84 42239 291.98
## - Leg      1   2664.65 43931 293.47
## - Age      1   2808.52 44075 293.59
## &lt;none&gt;                 41267 294.73
## 
## Step:  AIC=291.1
## hipcenter ~ Age + HtShoes + Seated + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Seated   1     35.10 41313 287.50
## - Arm      1    156.47 41434 287.61
## - Thigh    1    285.16 41563 287.73
## - HtShoes  1    975.48 42253 288.35
## - Leg      1   2661.39 43939 289.84
## - Age      1   3011.86 44290 290.14
## &lt;none&gt;                 41278 291.10
## 
## Step:  AIC=287.5
## hipcenter ~ Age + HtShoes + Arm + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Arm      1    172.02 41485 284.02
## - Thigh    1    344.61 41658 284.18
## - HtShoes  1   1853.43 43166 285.53
## - Leg      1   2871.07 44184 286.41
## - Age      1   2976.77 44290 286.50
## &lt;none&gt;                 41313 287.50
## 
## Step:  AIC=284.02
## hipcenter ~ Age + HtShoes + Thigh + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Thigh    1     472.8 41958 280.81
## - HtShoes  1    2340.7 43826 282.46
## - Age      1    3501.0 44986 283.46
## - Leg      1    3591.7 45077 283.54
## &lt;none&gt;                 41485 284.02
## 
## Step:  AIC=280.81
## hipcenter ~ Age + HtShoes + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Age      1    3108.8 45067 279.89
## - Leg      1    3476.3 45434 280.20
## &lt;none&gt;                 41958 280.81
## - HtShoes  1    4218.6 46176 280.81
## 
## Step:  AIC=279.89
## hipcenter ~ HtShoes + Leg
## 
##           Df Sum of Sq   RSS    AIC
## - Leg      1    3038.8 48105 278.73
## &lt;none&gt;                 45067 279.89
## - HtShoes  1    5004.4 50071 280.25
## 
## Step:  AIC=278.73
## hipcenter ~ HtShoes
## 
##           Df Sum of Sq    RSS    AIC
## &lt;none&gt;                  48105 278.73
## - HtShoes  1     83534 131639 313.35</code></pre>
<p>The procedure is exactly the same, except at each step we look to improve the <span class="math inline">\(\text{BIC}\)</span>, which <code>R</code> still labels <span class="math inline">\(\text{AIC}\)</span> in the output.</p>
<p>The variable <code>hipcenter_mod_back_bic</code> stores the model chosen by this procedure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(hipcenter_mod_back_bic)</code></pre></div>
<pre><code>## (Intercept)     HtShoes 
##  565.592659   -4.262091</code></pre>
<p>We note that this model is <em>smaller</em>, has fewer predictors, than the model chosen by <span class="math inline">\(\text{AIC}\)</span>, which is what we would expect. Also note that while both models are different, neither uses both <code>Ht</code> and <code>HtShoes</code> which are extremely correlated.</p>
<p>We can use information from the <code>summary()</code> function to compare their Adjusted <span class="math inline">\(R^2\)</span> values. Note that either selected model performs better than the original full model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6000855</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_back_aic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6531427</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_back_bic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6244149</code></pre>
<p>We can also calculate the LOOCV <span class="math inline">\(\text{RMSE}\)</span> for both selected models, as well as the full model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod)</code></pre></div>
<pre><code>## [1] 44.44564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_back_aic)</code></pre></div>
<pre><code>## [1] 37.58473</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_back_bic)</code></pre></div>
<pre><code>## [1] 37.40564</code></pre>
<p>We see that we would prefer the model chosen via <span class="math inline">\(\text{BIC}\)</span> if using LOOCV <span class="math inline">\(\text{RMSE}\)</span> as our metric.</p>
</div>
<div id="forward-search" class="section level3">
<h3><span class="header-section-number">15.2.2</span> Forward Search</h3>
<p>Forward selection is the exact opposite of backwards selection. Here we tell <code>R</code> to start with a model using no predictors, that is <code>hipcenter ~ 1</code>, then at each step <code>R</code> will attempt to add a predictor until it finds a good model or reaches <code>hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_start =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> seatpos)
hipcenter_mod_forw_aic =<span class="st"> </span><span class="kw">step</span>(
  hipcenter_mod_start, 
  <span class="dt">scope =</span> hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>HtShoes +<span class="st"> </span>Ht +<span class="st"> </span>Seated +<span class="st"> </span>Arm +<span class="st"> </span>Thigh +<span class="st"> </span>Leg, 
  <span class="dt">direction =</span> <span class="st">&quot;forward&quot;</span>)</code></pre></div>
<pre><code>## Start:  AIC=311.71
## hipcenter ~ 1
## 
##           Df Sum of Sq    RSS    AIC
## + Ht       1     84023  47616 275.07
## + HtShoes  1     83534  48105 275.45
## + Leg      1     81568  50071 276.98
## + Seated   1     70392  61247 284.63
## + Weight   1     53975  77664 293.66
## + Thigh    1     46010  85629 297.37
## + Arm      1     45065  86574 297.78
## &lt;none&gt;                 131639 311.71
## + Age      1      5541 126098 312.07
## 
## Step:  AIC=275.07
## hipcenter ~ Ht
## 
##           Df Sum of Sq   RSS    AIC
## + Leg      1   2781.10 44835 274.78
## &lt;none&gt;                 47616 275.07
## + Age      1   2353.51 45262 275.14
## + Weight   1    195.86 47420 276.91
## + Seated   1    101.56 47514 276.99
## + Arm      1     75.78 47540 277.01
## + HtShoes  1     25.76 47590 277.05
## + Thigh    1      4.63 47611 277.06
## 
## Step:  AIC=274.78
## hipcenter ~ Ht + Leg
## 
##           Df Sum of Sq   RSS    AIC
## + Age      1   2896.60 41938 274.24
## &lt;none&gt;                 44835 274.78
## + Arm      1    522.72 44312 276.33
## + Weight   1    445.10 44390 276.40
## + HtShoes  1     34.11 44801 276.75
## + Thigh    1     32.96 44802 276.75
## + Seated   1      1.12 44834 276.78
## 
## Step:  AIC=274.24
## hipcenter ~ Ht + Leg + Age
## 
##           Df Sum of Sq   RSS    AIC
## &lt;none&gt;                 41938 274.24
## + Thigh    1    372.71 41565 275.90
## + Arm      1    257.09 41681 276.01
## + Seated   1    121.26 41817 276.13
## + Weight   1     46.83 41891 276.20
## + HtShoes  1     13.38 41925 276.23</code></pre>
<p>Again, by default <code>R</code> uses <span class="math inline">\(\text{AIC}\)</span> as its quality metric when using the <code>step()</code> function. Also note that now the rows begin with a <code>+</code> which indicates addition of predictors to the current model from any step.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_forw_bic =<span class="st"> </span><span class="kw">step</span>(
  hipcenter_mod_start, 
  <span class="dt">scope =</span> hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>HtShoes +<span class="st"> </span>Ht +<span class="st"> </span>Seated +<span class="st"> </span>Arm +<span class="st"> </span>Thigh +<span class="st"> </span>Leg, 
  <span class="dt">direction =</span> <span class="st">&quot;forward&quot;</span>, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## Start:  AIC=313.35
## hipcenter ~ 1
## 
##           Df Sum of Sq    RSS    AIC
## + Ht       1     84023  47616 278.34
## + HtShoes  1     83534  48105 278.73
## + Leg      1     81568  50071 280.25
## + Seated   1     70392  61247 287.91
## + Weight   1     53975  77664 296.93
## + Thigh    1     46010  85629 300.64
## + Arm      1     45065  86574 301.06
## &lt;none&gt;                 131639 313.35
## + Age      1      5541 126098 315.35
## 
## Step:  AIC=278.34
## hipcenter ~ Ht
## 
##           Df Sum of Sq   RSS    AIC
## &lt;none&gt;                 47616 278.34
## + Leg      1   2781.10 44835 279.69
## + Age      1   2353.51 45262 280.05
## + Weight   1    195.86 47420 281.82
## + Seated   1    101.56 47514 281.90
## + Arm      1     75.78 47540 281.92
## + HtShoes  1     25.76 47590 281.96
## + Thigh    1      4.63 47611 281.98</code></pre>
<p>We can make the same modification as last time to instead use <span class="math inline">\(\text{BIC}\)</span> with forward selection.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6000855</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_forw_aic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6533055</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_forw_bic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6282374</code></pre>
<p>We can compare the two selected models’ Adjusted <span class="math inline">\(R^2\)</span> as well as their LOOCV <span class="math inline">\(\text{RMSE}\)</span> The results are very similar to those using backwards selection, although the models are not exactly the same.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod)</code></pre></div>
<pre><code>## [1] 44.44564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_forw_aic)</code></pre></div>
<pre><code>## [1] 37.62516</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_forw_bic)</code></pre></div>
<pre><code>## [1] 37.2511</code></pre>
</div>
<div id="stepwise-search" class="section level3">
<h3><span class="header-section-number">15.2.3</span> Stepwise Search</h3>
<p>Stepwise search checks going both backwards and forwards at every step. It considers the addition of any variable not currently in the model, as well as the removal of any variable currently in the model.</p>
<p>Here we perform stepwise search using <span class="math inline">\(\text{AIC}\)</span> as our metric. We start with the model <code>hipcenter ~ 1</code> and search up to <code>hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg</code>. Notice that at many of the steps, some row begin with <code>-</code>, while others begin with <code>+</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_both_aic =<span class="st"> </span><span class="kw">step</span>(
  hipcenter_mod_start, 
  <span class="dt">scope =</span> hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>HtShoes +<span class="st"> </span>Ht +<span class="st"> </span>Seated +<span class="st"> </span>Arm +<span class="st"> </span>Thigh +<span class="st"> </span>Leg, 
  <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>)</code></pre></div>
<pre><code>## Start:  AIC=311.71
## hipcenter ~ 1
## 
##           Df Sum of Sq    RSS    AIC
## + Ht       1     84023  47616 275.07
## + HtShoes  1     83534  48105 275.45
## + Leg      1     81568  50071 276.98
## + Seated   1     70392  61247 284.63
## + Weight   1     53975  77664 293.66
## + Thigh    1     46010  85629 297.37
## + Arm      1     45065  86574 297.78
## &lt;none&gt;                 131639 311.71
## + Age      1      5541 126098 312.07
## 
## Step:  AIC=275.07
## hipcenter ~ Ht
## 
##           Df Sum of Sq    RSS    AIC
## + Leg      1      2781  44835 274.78
## &lt;none&gt;                  47616 275.07
## + Age      1      2354  45262 275.14
## + Weight   1       196  47420 276.91
## + Seated   1       102  47514 276.99
## + Arm      1        76  47540 277.01
## + HtShoes  1        26  47590 277.05
## + Thigh    1         5  47611 277.06
## - Ht       1     84023 131639 311.71
## 
## Step:  AIC=274.78
## hipcenter ~ Ht + Leg
## 
##           Df Sum of Sq   RSS    AIC
## + Age      1    2896.6 41938 274.24
## &lt;none&gt;                 44835 274.78
## - Leg      1    2781.1 47616 275.07
## + Arm      1     522.7 44312 276.33
## + Weight   1     445.1 44390 276.40
## + HtShoes  1      34.1 44801 276.75
## + Thigh    1      33.0 44802 276.75
## + Seated   1       1.1 44834 276.78
## - Ht       1    5236.3 50071 276.98
## 
## Step:  AIC=274.24
## hipcenter ~ Ht + Leg + Age
## 
##           Df Sum of Sq   RSS    AIC
## &lt;none&gt;                 41938 274.24
## - Age      1    2896.6 44835 274.78
## - Leg      1    3324.2 45262 275.14
## - Ht       1    4238.3 46176 275.90
## + Thigh    1     372.7 41565 275.90
## + Arm      1     257.1 41681 276.01
## + Seated   1     121.3 41817 276.13
## + Weight   1      46.8 41891 276.20
## + HtShoes  1      13.4 41925 276.23</code></pre>
<p>We could again instead use <span class="math inline">\(\text{BIC}\)</span> as our metric.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_both_bic =<span class="st"> </span><span class="kw">step</span>(
  hipcenter_mod_start, 
  <span class="dt">scope =</span> hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>HtShoes +<span class="st"> </span>Ht +<span class="st"> </span>Seated +<span class="st"> </span>Arm +<span class="st"> </span>Thigh +<span class="st"> </span>Leg, 
  <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## Start:  AIC=313.35
## hipcenter ~ 1
## 
##           Df Sum of Sq    RSS    AIC
## + Ht       1     84023  47616 278.34
## + HtShoes  1     83534  48105 278.73
## + Leg      1     81568  50071 280.25
## + Seated   1     70392  61247 287.91
## + Weight   1     53975  77664 296.93
## + Thigh    1     46010  85629 300.64
## + Arm      1     45065  86574 301.06
## &lt;none&gt;                 131639 313.35
## + Age      1      5541 126098 315.35
## 
## Step:  AIC=278.34
## hipcenter ~ Ht
## 
##           Df Sum of Sq    RSS    AIC
## &lt;none&gt;                  47616 278.34
## + Leg      1      2781  44835 279.69
## + Age      1      2354  45262 280.05
## + Weight   1       196  47420 281.82
## + Seated   1       102  47514 281.90
## + Arm      1        76  47540 281.92
## + HtShoes  1        26  47590 281.96
## + Thigh    1         5  47611 281.98
## - Ht       1     84023 131639 313.35</code></pre>
<p>Adjusted <span class="math inline">\(R^2\)</span> and LOOCV <span class="math inline">\(\text{RMSE}\)</span> comparisons are similar to backwards and forwards, which is not at all surprising, as some of the models selected are the same as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6000855</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_both_aic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6533055</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hipcenter_mod_both_bic)$adj.r.squared</code></pre></div>
<pre><code>## [1] 0.6282374</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod)</code></pre></div>
<pre><code>## [1] 44.44564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_both_aic)</code></pre></div>
<pre><code>## [1] 37.62516</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(hipcenter_mod_both_bic)</code></pre></div>
<pre><code>## [1] 37.2511</code></pre>
</div>
<div id="exhaustive-search" class="section level3">
<h3><span class="header-section-number">15.2.4</span> Exhaustive Search</h3>
<p>Backward, forward, and stepwise search are all useful, but do have an obvious issue. By not checking every possible model, sometimes they will miss the best possible model. With an extremely large number of predictors, sometimes this is necessary since checking every possible model would be rather time consuming, even with current computers.</p>
<p>However, with a reasonably sized dataset, it isn’t too difficult to check all possible models. To do so, we will use the <code>regsubsets()</code> function in the <code>R</code> package <code>leaps</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(leaps)
all_hipcenter_mod =<span class="st"> </span><span class="kw">summary</span>(<span class="kw">regsubsets</span>(hipcenter ~<span class="st"> </span>., <span class="dt">data =</span> seatpos))</code></pre></div>
<p>A few points about this line of code. First, note that we immediately use <code>summary()</code> and store those results. That is simply the intended use of <code>regsubsets()</code>. Second, inside of <code>regsubsets()</code> we specify the model <code>hipcenter ~ .</code>. This will be the largest model considered, that is the model using all first-order predictors, and <code>R</code> will check all possible subsets.</p>
<p>We’ll now look at the information stored in <code>all_hipcenter_mod</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_hipcenter_mod$which</code></pre></div>
<pre><code>##   (Intercept)   Age Weight HtShoes    Ht Seated   Arm Thigh   Leg
## 1        TRUE FALSE  FALSE   FALSE  TRUE  FALSE FALSE FALSE FALSE
## 2        TRUE FALSE  FALSE   FALSE  TRUE  FALSE FALSE FALSE  TRUE
## 3        TRUE  TRUE  FALSE   FALSE  TRUE  FALSE FALSE FALSE  TRUE
## 4        TRUE  TRUE  FALSE    TRUE FALSE  FALSE FALSE  TRUE  TRUE
## 5        TRUE  TRUE  FALSE    TRUE FALSE  FALSE  TRUE  TRUE  TRUE
## 6        TRUE  TRUE  FALSE    TRUE FALSE   TRUE  TRUE  TRUE  TRUE
## 7        TRUE  TRUE   TRUE    TRUE FALSE   TRUE  TRUE  TRUE  TRUE
## 8        TRUE  TRUE   TRUE    TRUE  TRUE   TRUE  TRUE  TRUE  TRUE</code></pre>
<p>Using <code>$which</code> gives us the best model, according to <span class="math inline">\(\text{RSS}\)</span>, for a model of each possible size, in this case ranging from one to eight predictors. For example the best model with four predictors (<span class="math inline">\(p = 5\)</span>) would use <code>Age</code>, <code>HtShoes</code>, <code>Thigh</code>, and <code>Leg</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_hipcenter_mod$rss</code></pre></div>
<pre><code>## [1] 47615.79 44834.69 41938.09 41485.01 41313.00 41277.90 41266.80 41261.78</code></pre>
<p>We can obtain the <span class="math inline">\(\text{RSS}\)</span> for each of these models using <code>$rss</code>. Notice that these are decreasing since the models range from small to large.</p>
<p>Now that we have the <span class="math inline">\(\text{RSS}\)</span> for each of these models, it is rather easy to obtain <span class="math inline">\(\text{AIC}\)</span>, <span class="math inline">\(\text{BIC}\)</span>, and Adjusted <span class="math inline">\(R^2\)</span> since they are all a function of <span class="math inline">\(\text{RSS}\)</span> Also, since we have the models with the best <span class="math inline">\(\text{RSS}\)</span> for each size, they will result in the models with the best <span class="math inline">\(\text{AIC}\)</span>, <span class="math inline">\(\text{BIC}\)</span>, and Adjusted <span class="math inline">\(R^2\)</span> for each size. Then by picking from those, we can find the overall best <span class="math inline">\(\text{AIC}\)</span>, <span class="math inline">\(\text{BIC}\)</span>, and Adjusted <span class="math inline">\(R^2\)</span>.</p>
<p>Conveniently, Adjusted <span class="math inline">\(R^2\)</span> is automatically calculated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_hipcenter_mod$adjr2</code></pre></div>
<pre><code>## [1] 0.6282374 0.6399496 0.6533055 0.6466586 0.6371276 0.6257403 0.6133690
## [8] 0.6000855</code></pre>
<p>To find which model has the highest Adjusted <span class="math inline">\(R^2\)</span> we can use the <code>which.max()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">best_r2_ind =</span> <span class="kw">which.max</span>(all_hipcenter_mod$adjr2))</code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>We can then extract the predictors of that model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_hipcenter_mod$which[best_r2_ind, ]</code></pre></div>
<pre><code>## (Intercept)         Age      Weight     HtShoes          Ht      Seated 
##        TRUE        TRUE       FALSE       FALSE        TRUE       FALSE 
##         Arm       Thigh         Leg 
##       FALSE       FALSE        TRUE</code></pre>
<p>We’ll now calculate <span class="math inline">\(\text{AIC}\)</span> and <span class="math inline">\(\text{BIC}\)</span> for the each of the models with the best <span class="math inline">\(\text{RSS}\)</span>. To do so, we will need both <span class="math inline">\(n\)</span> and the <span class="math inline">\(p\)</span> for the largest possible model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p =<span class="st"> </span><span class="kw">length</span>(<span class="kw">coef</span>(hipcenter_mod))
n =<span class="st"> </span><span class="kw">length</span>(<span class="kw">resid</span>(hipcenter_mod))</code></pre></div>
<p>We’ll use the form of <span class="math inline">\(\text{AIC}\)</span> which leaves out the constant term that is equal across all models.</p>
<p><span class="math display">\[
\text{AIC} = n\log\left(\frac{\text{RSS}}{n}\right) + 2p.
\]</span></p>
<p>Since we have the <span class="math inline">\(\text{RSS}\)</span> of each model stored, this is easy to calculate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_aic =<span class="st"> </span>n *<span class="st"> </span><span class="kw">log</span>(all_hipcenter_mod$rss /<span class="st"> </span>n) +<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>(<span class="dv">2</span>:p)</code></pre></div>
<p>We can then extract the predictors of the model with the best <span class="math inline">\(\text{AIC}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_aic_ind =<span class="st"> </span><span class="kw">which.min</span>(hipcenter_mod_aic)
all_hipcenter_mod$which[best_aic_ind,]</code></pre></div>
<pre><code>## (Intercept)         Age      Weight     HtShoes          Ht      Seated 
##        TRUE        TRUE       FALSE       FALSE        TRUE       FALSE 
##         Arm       Thigh         Leg 
##       FALSE       FALSE        TRUE</code></pre>
<p>Let’s fit this model so we can compare to our previously chosen models using <span class="math inline">\(\text{AIC}\)</span> and search procedures.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_best_aic =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Ht +<span class="st"> </span>Leg, <span class="dt">data =</span> seatpos)</code></pre></div>
<p>The <code>extractAIC()</code> function will calculate the <span class="math inline">\(\text{AIC}\)</span> defined above for a fitted model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_best_aic)</code></pre></div>
<pre><code>## [1]   4.0000 274.2418</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_back_aic)</code></pre></div>
<pre><code>## [1]   4.0000 274.2597</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_forw_aic)</code></pre></div>
<pre><code>## [1]   4.0000 274.2418</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_both_aic)</code></pre></div>
<pre><code>## [1]   4.0000 274.2418</code></pre>
<p>We see that two of the models chosen by search procedures have the best possible <span class="math inline">\(\text{AIC}\)</span>, as they are the same model. This is however never guaranteed. We see that the model chosen using backwards selection does not achieve the smallest possible <span class="math inline">\(\text{AIC}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(hipcenter_mod_aic ~<span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span>:p), <span class="dt">ylab =</span> <span class="st">&quot;AIC&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p, number of parameters&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">cex =</span> <span class="dv">2</span>,
     <span class="dt">main =</span> <span class="st">&quot;AIC vs Model Complexity&quot;</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-529-1.png" width="672" /></p>
<p>We could easily repeat this process for <span class="math inline">\(\text{BIC}\)</span>.</p>
<p><span class="math display">\[
\text{BIC} = n\log\left(\frac{\text{RSS}}{n}\right) + \log(n)p.
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_bic =<span class="st"> </span>n *<span class="st"> </span><span class="kw">log</span>(all_hipcenter_mod$rss /<span class="st"> </span>n) +<span class="st"> </span><span class="kw">log</span>(n) *<span class="st"> </span>(<span class="dv">2</span>:p)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">which.min</span>(hipcenter_mod_bic)</code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_hipcenter_mod$which[<span class="dv">1</span>,]</code></pre></div>
<pre><code>## (Intercept)         Age      Weight     HtShoes          Ht      Seated 
##        TRUE       FALSE       FALSE       FALSE        TRUE       FALSE 
##         Arm       Thigh         Leg 
##       FALSE       FALSE       FALSE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hipcenter_mod_best_bic =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span>Ht, <span class="dt">data =</span> seatpos)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_best_bic, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## [1]   2.0000 278.3418</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_back_bic, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## [1]   2.0000 278.7306</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_forw_bic, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## [1]   2.0000 278.3418</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(hipcenter_mod_both_bic, <span class="dt">k =</span> <span class="kw">log</span>(n))</code></pre></div>
<pre><code>## [1]   2.0000 278.3418</code></pre>
</div>
</div>
<div id="higher-order-terms" class="section level2">
<h2><span class="header-section-number">15.3</span> Higher Order Terms</h2>
<p>So far we have only allowed first-order terms in our models. Let’s return to the <code>autompg</code> dataset to explore higher-order terms.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">autompg =<span class="st"> </span><span class="kw">read.table</span>(
  <span class="st">&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;</span>,
  <span class="dt">quote =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">&quot;</span>,
  <span class="dt">comment.char =</span> <span class="st">&quot;&quot;</span>,
  <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
<span class="kw">colnames</span>(autompg) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;acc&quot;</span>, 
                      <span class="st">&quot;year&quot;</span>, <span class="st">&quot;origin&quot;</span>, <span class="st">&quot;name&quot;</span>)
autompg =<span class="st"> </span><span class="kw">subset</span>(autompg, autompg$hp !=<span class="st"> &quot;?&quot;</span>)
autompg =<span class="st"> </span><span class="kw">subset</span>(autompg, autompg$name !=<span class="st"> &quot;plymouth reliant&quot;</span>)
<span class="kw">rownames</span>(autompg) =<span class="st"> </span><span class="kw">paste</span>(autompg$cyl, <span class="st">&quot;cylinder&quot;</span>, autompg$year, autompg$name)
autompg$hp =<span class="st"> </span><span class="kw">as.numeric</span>(autompg$hp)
autompg$domestic =<span class="st"> </span><span class="kw">as.numeric</span>(autompg$origin ==<span class="st"> </span><span class="dv">1</span>)
autompg =<span class="st"> </span>autompg[autompg$cyl !=<span class="st"> </span><span class="dv">5</span>,]
autompg =<span class="st"> </span>autompg[autompg$cyl !=<span class="st"> </span><span class="dv">3</span>,]
autompg$cyl =<span class="st"> </span><span class="kw">as.factor</span>(autompg$cyl)
autompg$domestic =<span class="st"> </span><span class="kw">as.factor</span>(autompg$domestic)
autompg =<span class="st"> </span><span class="kw">subset</span>(autompg, <span class="dt">select =</span> <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, 
                                     <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;acc&quot;</span>, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;domestic&quot;</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(autompg)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    383 obs. of  8 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ domestic: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Recall that we have two factor variables, <code>cyl</code> and <code>domestic</code>. The <code>cyl</code> variable has three levels, while the <code>domestic</code> variable has only two. Thus the <code>cyl</code> variable will be coded using two dummy variables, while the <code>domestic</code> variable will only need one. We will pay attention to this later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairs</span>(autompg, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-536-1.png" width="768" /></p>
<p>We’ll use the <code>pairs()</code> plot to determine which variables may benefit from a quadratic relationship with the response. We’ll also consider all possible two-way interactions. We won’t consider any three-order or higher. For example, we won’t consider the interaction between first-order terms and the added quadratic terms.</p>
<p>So now, we’ll fit this rather large model. We’ll use a log-transformed response. Notice that <code>log(mpg) ~ . ^ 2</code> will automatically consider all first-order terms, as well as all two-way interactions. We use <code>I(var_name ^ 2)</code> to add quadratic terms for some variables. This generally works better than using <code>poly()</code> when performing variable selection.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">autompg_big_mod =<span class="st"> </span><span class="kw">lm</span>(
  <span class="kw">log</span>(mpg) ~<span class="st"> </span>. ^<span class="st"> </span><span class="dv">2</span> +<span class="st"> </span><span class="kw">I</span>(disp ^<span class="st"> </span><span class="dv">2</span>) +<span class="st"> </span><span class="kw">I</span>(hp ^<span class="st"> </span><span class="dv">2</span>) +<span class="st"> </span><span class="kw">I</span>(wt ^<span class="st"> </span><span class="dv">2</span>) +<span class="st"> </span><span class="kw">I</span>(acc ^<span class="st"> </span><span class="dv">2</span>), 
  <span class="dt">data =</span> autompg)</code></pre></div>
<p>We think it is rather unlikely that we truly need all of these terms. There are quite a few!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">coef</span>(autompg_big_mod))</code></pre></div>
<pre><code>## [1] 40</code></pre>
<p>We’ll try backwards search with both <span class="math inline">\(\text{AIC}\)</span> and <span class="math inline">\(\text{BIC}\)</span> to attempt to find a smaller, more reasonable model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">autompg_mod_back_aic =<span class="st"> </span><span class="kw">step</span>(autompg_big_mod, <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>, <span class="dt">trace =</span> <span class="dv">0</span>)</code></pre></div>
<p>Notice that we used <code>trace = 0</code> in the function call. This suppress the output for each step, and simply stores the chosen model. This is useful, as this code would otherwise create a large amount of output. If we had viewed the output, which you can try on your own by removing <code>trace = 0</code>, we would see that <code>R</code> only considers the <code>cyl</code> variable as a single variable, despite the fact that it is coded using two dummy variables. So removing <code>cyl</code> would actually remove two parameters from the resulting model.</p>
<p>You should also notice that <code>R</code> respects hierarchy when attempting to remove variables. That is, for example, <code>R</code> will not consider removing <code>hp</code> if <code>hp:disp</code> or <code>I(hp ^ 2)</code> are currently in the model.</p>
<p>We also use <span class="math inline">\(\text{BIC}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="kw">length</span>(<span class="kw">resid</span>(autompg_big_mod))
autompg_mod_back_bic =<span class="st"> </span><span class="kw">step</span>(autompg_big_mod, <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>, 
                            <span class="dt">k =</span> <span class="kw">log</span>(n), <span class="dt">trace =</span> <span class="dv">0</span>)</code></pre></div>
<p>Looking at the coefficients of the two chosen models, we see they are still rather large.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(autompg_mod_back_aic)</code></pre></div>
<pre><code>##      (Intercept)             cyl6             cyl8             disp 
##  3.6718839511472 -0.1602563336333 -0.8581644338393 -0.0093719707873 
##               hp               wt              acc             year 
##  0.0229353409493 -0.0003064496949 -0.1393888479750 -0.0019663606344 
##        domestic1          I(hp^2)         cyl6:acc         cyl8:acc 
##  0.9369323953411 -0.0000149766900  0.0072202979695  0.0504191492217 
##          disp:wt        disp:year           hp:acc          hp:year 
##  0.0000005797816  0.0000949376953 -0.0005062294609 -0.0001838985017 
##         acc:year    acc:domestic1   year:domestic1 
##  0.0023456252781 -0.0237246840975 -0.0073327246317</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(autompg_mod_back_bic)</code></pre></div>
<pre><code>##      (Intercept)             cyl6             cyl8             disp 
##  4.6578470390516 -0.1086165018406 -0.7611630775038 -0.0016093164199 
##               hp               wt              acc             year 
##  0.0026212660296 -0.0002635971659 -0.1670601021732 -0.0104564626969 
##        domestic1         cyl6:acc         cyl8:acc          disp:wt 
##  0.3341578960873  0.0043154925327  0.0461009496981  0.0000004102804 
##           hp:acc         acc:year    acc:domestic1 
## -0.0003386261424  0.0025001372156 -0.0219329407492</code></pre>
<p>However, they are much smaller than the original full model. Also notice that the resulting models respect hierarchy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">coef</span>(autompg_big_mod))</code></pre></div>
<pre><code>## [1] 40</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">coef</span>(autompg_mod_back_aic))</code></pre></div>
<pre><code>## [1] 19</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">coef</span>(autompg_mod_back_bic))</code></pre></div>
<pre><code>## [1] 15</code></pre>
<p>Calculating the LOOCV <span class="math inline">\(\text{RMSE}\)</span> for each, we see that the model chosen using <span class="math inline">\(\text{BIC}\)</span> performs the best. That means that it is both the best model for prediction, since it achieves the best LOOCV <span class="math inline">\(\text{RMSE}\)</span>, but also the best model for explanation, as it is also the smallest.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(autompg_big_mod)</code></pre></div>
<pre><code>## [1] 0.1112024</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(autompg_mod_back_aic)</code></pre></div>
<pre><code>## [1] 0.1032888</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">calc_loocv_rmse</span>(autompg_mod_back_bic)</code></pre></div>
<pre><code>## [1] 0.103134</code></pre>
</div>
<div id="explanation-versus-prediction-1" class="section level2">
<h2><span class="header-section-number">15.4</span> Explanation versus Prediction</h2>
<p>Throughout this chapter, we have attempted to find reasonably “small” models, which are good at <strong>explaining</strong> the relationship between the response and the predictors, that also have small errors which are thus good for making <strong>predictions</strong>.</p>
<p>We’ll further discuss the model <code>autompg_mod_back_bic</code> to better explain the difference between using models for <em>explaining</em> and <em>predicting</em>. This is the model fit to the <code>autompg</code> data that was chosen using Backwards Search and <span class="math inline">\(\text{BIC}\)</span>, which obtained the lowest LOOCV <span class="math inline">\(\text{RMSE}\)</span> of the models we considered.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">autompg_mod_back_bic</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(mpg) ~ cyl + disp + hp + wt + acc + year + domestic + 
##     cyl:acc + disp:wt + hp:acc + acc:year + acc:domestic, data = autompg)
## 
## Coefficients:
##   (Intercept)           cyl6           cyl8           disp             hp  
##  4.6578470391  -0.1086165018  -0.7611630775  -0.0016093164   0.0026212660  
##            wt            acc           year      domestic1       cyl6:acc  
## -0.0002635972  -0.1670601022  -0.0104564627   0.3341578961   0.0043154925  
##      cyl8:acc        disp:wt         hp:acc       acc:year  acc:domestic1  
##  0.0461009497   0.0000004103  -0.0003386261   0.0025001372  -0.0219329407</code></pre>
<p>Notice this is a somewhat “large” model, which uses 15 parameters, including several interaction terms. Do we care that this is a “large” model? The answer is, <strong>it depends</strong>.</p>
<div id="explanation-1" class="section level3">
<h3><span class="header-section-number">15.4.1</span> Explanation</h3>
<p>Suppose we would like to use this model for explanation. Perhaps we are a car manufacturer trying to engineer a fuel efficient vehicle. If this is the case, we are interested in both what predictor variables are useful for explaining the car’s fuel efficiency, as well as how those variables effect fuel efficiency. By understanding this relationship, we can use this knowledge to our advantage when designing a car.</p>
<p>To explain a relationship, we are interested in keeping models as small as possible, since smaller models are easy to interpret. The fewer predictors the less considerations we need to make in our design process. Also the fewer interactions and polynomial terms, the easier it is to interpret any one parameter, since the parameter interpretations are conditional on which parameters are in the model.</p>
<p>Note that <em>linear</em> models are rather interpretable to begin with. Later in your data analysis careers, you will see more complicated models that may fit data better, but are much harder, if not impossible to interpret. These models aren’t very useful for explaining a relationship.</p>
<p>To find small and interpretable models, we would use selection criterion that <em>explicitly</em> penalize larger models, such as AIC and BIC. In this case we still obtained a somewhat large model, but much smaller than the model we used to start the selection process.</p>
<div id="correlation-and-causation-1" class="section level4">
<h4><span class="header-section-number">15.4.1.1</span> Correlation and Causation</h4>
<p>A word of caution when using a model to <em>explain</em> a relationship. There are two terms often used to describe a relationship between two variables: <em>causation</em> and <em>correlation</em>. <a href="https://xkcd.com/552/">Correlation</a> is often also referred to as association.</p>
<p>Just because two variable are correlated does not necessarily mean that one causes the other. For example, considering modeling <code>mpg</code> as only a function of <code>hp</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mpg ~<span class="st"> </span>hp, <span class="dt">data =</span> autompg, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-545-1.png" width="672" /></p>
<p>Does an increase in horsepower cause a drop in fuel efficiency? Or, perhaps the causality is reversed and an increase in fuel efficiency cause a decrease in horsepower. Or, perhaps there is a third variable that explains both!</p>
<p>The issue here is that we have <strong>observational</strong> data. With observational data, we can only detect associations. To speak with confidence about causality, we would need to run <strong>experiments</strong>.</p>
<p>This is a concept that you should encounter often in your statistics education. For some further reading, and some related fallacies, see: <a href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">Wikipedia: Correlation does not imply causation</a>.</p>
</div>
</div>
<div id="prediction-1" class="section level3">
<h3><span class="header-section-number">15.4.2</span> Prediction</h3>
<p>Suppose now instead of the manufacturer who would like to build a car, we are a consumer who wishes to purchase a new car. However this particular car is so new, it has not been rigorously tested, so we are unsure of what fuel efficiency to expect. (And, as skeptics, we don’t trust what the manufacturer is telling us.)</p>
<p>In this case, we would like to use the model to help <em>predict</em> the fuel efficiency of this car based on its attributes, which are the predictors of the model. The smaller the errors the model makes, the more confident we are in its prediction. Thus, to find models for prediction, we would use selection criterion that <em>implicitly</em> penalize larger models, such as LOOCV <span class="math inline">\(\text{RMSE}\)</span>. So long as the model does not over-fit, we do not actually care how large the model becomes. Explaining the relationship between the variables is not our goal here, we simply want to know what kind of fuel efficiency we should expect!</p>
<p>If we <strong>only</strong> care about prediction, we don’t need to worry about correlation vs causation, and we don’t need to worry about model assumptions.</p>
<p>If a variable is correlated with the response, it doesn’t actually matter if it causes an effect on the response, it can still be useful for prediction. For example, in elementary school aged children their shoe size certainly doesn’t <em>cause</em> them to read at a higher level, however we could very easily use shoe size to make a prediction about a child’s reading ability. The larger their shoe size, the better they read. There’s a lurking variable here though, their age! (Don’t send your kids to school with size 14 shoes, it won’t make them read better!)</p>
<p>We also don’t care about model assumptions. Least squares is least squares. For a specified model, it will find the values of the parameters which will minimize the squared error loss. Your results might be largely uninterpretable and useless for inference, but for prediction none of that matters.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="collinearity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="beyond.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/appliedstats/edit/master/selection.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
